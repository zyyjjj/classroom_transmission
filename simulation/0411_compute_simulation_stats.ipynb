{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from simulation import Simulation\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\IPython\\core\\interactiveshell.py:3553: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "g:\\Data_Peter\\classroom_transmission\\simulation\\simulation.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.df['day_idx'] = self.df['day_idx'].apply(lambda x: dt.datetime.strptime(x, '%Y-%m-%d').toordinal() - baseline_date)\n",
      "g:\\Data_Peter\\classroom_transmission\\simulation\\simulation.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.df['day_idx_stop'] = self.df['day_idx'] + 1\n"
     ]
    }
   ],
   "source": [
    "all_students_features_sp22 = pd.read_csv('../data/data_sp22/all_students_features_T_e=7_finalized_nontruncated_sp22.csv', index_col = 0)\n",
    "\n",
    "sim = Simulation(student_features=all_students_features_sp22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUE_COEF = 0.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape value 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_VM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_VM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_VM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_VM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.171. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0, cox coeff=-2.34, p=0.39, logit coeff=-2.34, p=0.34; logit campus_pos coeff=0.01, p=0.10; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GR have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GR'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GR'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GR completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.369. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1, cox coeff=0.82, p=0.79, logit coeff=0.82, p=0.77; logit campus_pos coeff=-0.01, p=0.07; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GR have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GR'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GR'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GR completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.370. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2, cox coeff=-4.90, p=0.09, logit coeff=-4.90, p=0.07; logit campus_pos coeff=-0.02, p=0.00; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_UG_A have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_UG_A'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_UG_A'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_UG_A completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.208. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3, cox coeff=-2.86, p=0.33, logit coeff=-2.86, p=0.38; logit campus_pos coeff=-0.01, p=0.14; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_UG_A have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_UG_A'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_UG_A'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_UG_A completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.264. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 4, cox coeff=-0.75, p=0.77, logit coeff=-0.75, p=0.77; logit campus_pos coeff=-0.00, p=0.21; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GR have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GR'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GR'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GR completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.362. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 5, cox coeff=-0.36, p=0.87, logit coeff=-0.36, p=0.87; logit campus_pos coeff=-0.00, p=0.90; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_LA have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_LA'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_LA'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_LA completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.204. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 6, cox coeff=0.15, p=0.95, logit coeff=0.15, p=0.95; logit campus_pos coeff=0.00, p=0.27; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_VM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_VM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_VM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_VM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.171. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 7, cox coeff=-3.44, p=0.19, logit coeff=-3.44, p=0.16; logit campus_pos coeff=-0.01, p=0.05; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_LA have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_LA'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_LA'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_LA completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.205. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 8, cox coeff=-4.48, p=0.14, logit coeff=-4.49, p=0.10; logit campus_pos coeff=-0.01, p=0.06; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_LA have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_LA'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_LA'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_LA completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.261. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 9, cox coeff=-3.50, p=0.16, logit coeff=-3.51, p=0.18; logit campus_pos coeff=-0.00, p=0.28; \n",
      "batch 10, cox coeff=4.09, p=0.10, logit coeff=4.10, p=0.06; logit campus_pos coeff=-0.00, p=0.32; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_VM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_VM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_VM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_VM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.171. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 11, cox coeff=2.10, p=0.38, logit coeff=2.10, p=0.36; logit campus_pos coeff=-0.00, p=0.29; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_VM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_VM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_VM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_VM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.164. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 12, cox coeff=-2.73, p=0.38, logit coeff=-2.73, p=0.40; logit campus_pos coeff=-0.01, p=0.01; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.270. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 13, cox coeff=0.17, p=0.95, logit coeff=0.17, p=0.96; logit campus_pos coeff=-0.01, p=0.04; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.352. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 14, cox coeff=-4.52, p=0.12, logit coeff=-4.53, p=0.11; logit campus_pos coeff=-0.01, p=0.01; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_VM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_VM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_VM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_VM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.164. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 15, cox coeff=0.18, p=0.95, logit coeff=0.17, p=0.95; logit campus_pos coeff=-0.01, p=0.24; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_VM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_VM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_VM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_VM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.171. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 16, cox coeff=-0.15, p=0.95, logit coeff=-0.15, p=0.95; logit campus_pos coeff=-0.00, p=0.72; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_UG_A have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_UG_A'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_UG_A'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_UG_A completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.264. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 17, cox coeff=1.40, p=0.58, logit coeff=1.40, p=0.56; logit campus_pos coeff=-0.00, p=0.27; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_LA have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_LA'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_LA'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_LA completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.204. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 18, cox coeff=0.84, p=0.74, logit coeff=0.84, p=0.75; logit campus_pos coeff=-0.01, p=0.03; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GR have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GR'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GR'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GR completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.308. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 19, cox coeff=3.85, p=0.10, logit coeff=3.85, p=0.10; logit campus_pos coeff=-0.01, p=0.02; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.166. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 20, cox coeff=3.52, p=0.20, logit coeff=3.52, p=0.23; logit campus_pos coeff=-0.01, p=0.01; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.166. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 21, cox coeff=-1.71, p=0.59, logit coeff=-1.71, p=0.57; logit campus_pos coeff=-0.00, p=0.72; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_UG_A have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_UG_A'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_UG_A'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_UG_A completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.208. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 22, cox coeff=1.34, p=0.57, logit coeff=1.34, p=0.55; logit campus_pos coeff=-0.00, p=0.36; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.243. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 23, cox coeff=0.06, p=0.98, logit coeff=0.06, p=0.98; logit campus_pos coeff=-0.00, p=0.96; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_UG_A have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_UG_A'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_UG_A'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_UG_A completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.208. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 24, cox coeff=-1.07, p=0.70, logit coeff=-1.07, p=0.71; logit campus_pos coeff=-0.01, p=0.05; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GR have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GR'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GR'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GR completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.264. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 25, cox coeff=-0.89, p=0.75, logit coeff=-0.89, p=0.76; logit campus_pos coeff=-0.01, p=0.20; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GR have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GR'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GR'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GR completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.308. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 26, cox coeff=2.08, p=0.41, logit coeff=2.08, p=0.41; logit campus_pos coeff=-0.00, p=0.51; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_UG_A have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_UG_A'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_UG_A'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_UG_A completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.264. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 27, cox coeff=-4.20, p=0.15, logit coeff=-4.21, p=0.13; logit campus_pos coeff=-0.00, p=0.30; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_VM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_VM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_VM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_VM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.171. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 28, cox coeff=0.07, p=0.98, logit coeff=0.07, p=0.97; logit campus_pos coeff=-0.00, p=0.26; \n",
      "batch 29, cox coeff=1.32, p=0.58, logit coeff=1.32, p=0.51; logit campus_pos coeff=-0.01, p=0.18; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_UG_A have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_UG_A'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_UG_A'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_UG_A completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.208. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 30, cox coeff=-0.22, p=0.93, logit coeff=-0.22, p=0.93; logit campus_pos coeff=-0.00, p=0.38; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GR have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GR'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GR'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GR completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.308. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 31, cox coeff=-0.58, p=0.85, logit coeff=-0.59, p=0.88; logit campus_pos coeff=0.00, p=0.27; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GR have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GR'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GR'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GR completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.308. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 32, cox coeff=-1.12, p=0.69, logit coeff=-1.12, p=0.67; logit campus_pos coeff=-0.01, p=0.00; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_LA have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_LA'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_LA'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_LA completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.204. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 33, cox coeff=-1.10, p=0.71, logit coeff=-1.10, p=0.71; logit campus_pos coeff=-0.00, p=0.95; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_VM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_VM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_VM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_VM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.171. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 34, cox coeff=2.26, p=0.36, logit coeff=2.26, p=0.34; logit campus_pos coeff=-0.01, p=0.18; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GR have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GR'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GR'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GR completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.259. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 35, cox coeff=1.56, p=0.49, logit coeff=1.57, p=0.45; logit campus_pos coeff=-0.00, p=0.17; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GR have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GR'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GR'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GR completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.264. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 36, cox coeff=1.48, p=0.60, logit coeff=1.48, p=0.63; logit campus_pos coeff=0.00, p=0.56; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_LA have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_LA'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_LA'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_LA completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.292. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 37, cox coeff=3.62, p=0.19, logit coeff=3.62, p=0.20; logit campus_pos coeff=-0.01, p=0.10; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GR have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GR'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GR'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GR completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.369. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 38, cox coeff=0.33, p=0.91, logit coeff=0.33, p=0.92; logit campus_pos coeff=-0.00, p=0.23; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_LA have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_LA'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_LA'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_LA completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.261. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 39, cox coeff=4.31, p=0.06, logit coeff=4.31, p=0.05; logit campus_pos coeff=-0.01, p=0.00; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.352. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 40, cox coeff=-0.61, p=0.82, logit coeff=-0.61, p=0.82; logit campus_pos coeff=-0.01, p=0.14; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.320. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 41, cox coeff=3.58, p=0.24, logit coeff=3.58, p=0.30; logit campus_pos coeff=-0.00, p=0.52; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.166. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 42, cox coeff=1.55, p=0.57, logit coeff=1.55, p=0.53; logit campus_pos coeff=-0.01, p=0.24; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GR have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GR'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GR'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GR completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.308. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 43, cox coeff=-2.99, p=0.22, logit coeff=-2.99, p=0.22; logit campus_pos coeff=0.00, p=0.72; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_VM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_VM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_VM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_VM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.171. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 44, cox coeff=3.39, p=0.17, logit coeff=3.39, p=0.17; logit campus_pos coeff=-0.00, p=0.31; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.166. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 45, cox coeff=1.84, p=0.49, logit coeff=1.84, p=0.46; logit campus_pos coeff=-0.00, p=0.20; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_UG_A have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_UG_A'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_UG_A'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_UG_A completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.264. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 46, cox coeff=-3.72, p=0.20, logit coeff=-3.72, p=0.23; logit campus_pos coeff=-0.00, p=0.34; \n",
      "batch 47, cox coeff=6.99, p=0.00, logit coeff=6.99, p=0.00; logit campus_pos coeff=-0.01, p=0.05; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_UG_A have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_UG_A'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_UG_A'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_UG_A completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.264. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 48, cox coeff=-0.42, p=0.85, logit coeff=-0.42, p=0.85; logit campus_pos coeff=-0.00, p=0.29; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.238. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 49, cox coeff=1.28, p=0.58, logit coeff=1.28, p=0.56; logit campus_pos coeff=-0.00, p=0.35; \n",
      "shape value 1.0\n",
      "batch 0, cox coeff=-0.14, p=0.91, logit coeff=-0.14, p=0.90; logit campus_pos coeff=0.00, p=0.44; \n",
      "batch 1, cox coeff=3.16, p=0.01, logit coeff=3.17, p=0.01; logit campus_pos coeff=-0.00, p=0.25; \n",
      "batch 2, cox coeff=-0.28, p=0.82, logit coeff=-0.28, p=0.80; logit campus_pos coeff=-0.00, p=0.21; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_VM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_VM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_VM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_VM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.168. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3, cox coeff=1.43, p=0.23, logit coeff=1.43, p=0.23; logit campus_pos coeff=-0.00, p=0.83; \n",
      "batch 4, cox coeff=2.41, p=0.04, logit coeff=2.41, p=0.04; logit campus_pos coeff=0.00, p=0.44; \n",
      "batch 5, cox coeff=1.33, p=0.28, logit coeff=1.34, p=0.26; logit campus_pos coeff=0.00, p=0.82; \n",
      "batch 6, cox coeff=0.35, p=0.78, logit coeff=0.35, p=0.79; logit campus_pos coeff=0.00, p=0.74; \n",
      "batch 7, cox coeff=-2.04, p=0.12, logit coeff=-2.04, p=0.12; logit campus_pos coeff=0.00, p=0.33; \n",
      "batch 8, cox coeff=0.93, p=0.47, logit coeff=0.94, p=0.48; logit campus_pos coeff=0.00, p=0.91; \n",
      "batch 9, cox coeff=-0.10, p=0.93, logit coeff=-0.10, p=0.93; logit campus_pos coeff=-0.00, p=0.22; \n",
      "batch 10, cox coeff=-2.39, p=0.09, logit coeff=-2.39, p=0.10; logit campus_pos coeff=0.00, p=0.45; \n",
      "batch 11, cox coeff=1.98, p=0.10, logit coeff=1.98, p=0.11; logit campus_pos coeff=0.00, p=0.65; \n",
      "batch 12, cox coeff=-1.40, p=0.27, logit coeff=-1.40, p=0.24; logit campus_pos coeff=0.00, p=0.25; \n",
      "batch 13, cox coeff=3.91, p=0.00, logit coeff=3.91, p=0.00; logit campus_pos coeff=0.00, p=0.94; \n",
      "batch 14, cox coeff=-0.79, p=0.56, logit coeff=-0.79, p=0.56; logit campus_pos coeff=-0.00, p=0.07; \n",
      "batch 15, cox coeff=1.91, p=0.10, logit coeff=1.91, p=0.07; logit campus_pos coeff=-0.00, p=0.30; \n",
      "batch 16, cox coeff=0.36, p=0.77, logit coeff=0.36, p=0.77; logit campus_pos coeff=0.00, p=0.91; \n",
      "batch 17, cox coeff=-1.47, p=0.26, logit coeff=-1.47, p=0.24; logit campus_pos coeff=0.00, p=0.26; \n",
      "batch 18, cox coeff=1.18, p=0.34, logit coeff=1.18, p=0.34; logit campus_pos coeff=-0.00, p=0.17; \n",
      "batch 19, cox coeff=1.15, p=0.34, logit coeff=1.15, p=0.36; logit campus_pos coeff=0.00, p=0.41; \n",
      "batch 20, cox coeff=-0.05, p=0.97, logit coeff=-0.05, p=0.97; logit campus_pos coeff=-0.00, p=0.01; \n",
      "batch 21, cox coeff=-0.72, p=0.58, logit coeff=-0.72, p=0.58; logit campus_pos coeff=-0.00, p=0.50; \n",
      "batch 22, cox coeff=0.58, p=0.62, logit coeff=0.58, p=0.61; logit campus_pos coeff=-0.00, p=0.35; \n",
      "batch 23, cox coeff=-0.47, p=0.69, logit coeff=-0.47, p=0.69; logit campus_pos coeff=0.00, p=0.45; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_VM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_VM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_VM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_VM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.168. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 24, cox coeff=1.82, p=0.13, logit coeff=1.82, p=0.13; logit campus_pos coeff=0.00, p=0.48; \n",
      "batch 25, cox coeff=1.15, p=0.34, logit coeff=1.15, p=0.33; logit campus_pos coeff=-0.00, p=0.68; \n",
      "batch 26, cox coeff=-1.58, p=0.25, logit coeff=-1.58, p=0.27; logit campus_pos coeff=-0.00, p=0.68; \n",
      "batch 27, cox coeff=-1.85, p=0.16, logit coeff=-1.85, p=0.13; logit campus_pos coeff=0.00, p=0.04; \n",
      "batch 28, cox coeff=1.31, p=0.27, logit coeff=1.31, p=0.26; logit campus_pos coeff=0.00, p=0.20; \n",
      "batch 29, cox coeff=1.54, p=0.19, logit coeff=1.55, p=0.18; logit campus_pos coeff=-0.00, p=0.86; \n",
      "batch 30, cox coeff=0.52, p=0.68, logit coeff=0.52, p=0.71; logit campus_pos coeff=0.00, p=0.70; \n",
      "batch 31, cox coeff=0.94, p=0.47, logit coeff=0.94, p=0.47; logit campus_pos coeff=0.00, p=0.61; \n",
      "batch 32, cox coeff=0.29, p=0.82, logit coeff=0.29, p=0.82; logit campus_pos coeff=-0.00, p=0.69; \n",
      "batch 33, cox coeff=-0.03, p=0.98, logit coeff=-0.03, p=0.98; logit campus_pos coeff=0.00, p=0.65; \n",
      "batch 34, cox coeff=1.28, p=0.28, logit coeff=1.28, p=0.28; logit campus_pos coeff=-0.00, p=0.74; \n",
      "batch 35, cox coeff=0.01, p=0.99, logit coeff=0.01, p=0.99; logit campus_pos coeff=0.00, p=0.05; \n",
      "batch 36, cox coeff=0.67, p=0.62, logit coeff=0.67, p=0.60; logit campus_pos coeff=0.00, p=0.56; \n",
      "batch 37, cox coeff=0.64, p=0.62, logit coeff=0.64, p=0.62; logit campus_pos coeff=0.00, p=0.01; \n",
      "batch 38, cox coeff=-0.15, p=0.90, logit coeff=-0.15, p=0.90; logit campus_pos coeff=-0.00, p=0.28; \n",
      "batch 39, cox coeff=2.60, p=0.03, logit coeff=2.60, p=0.03; logit campus_pos coeff=-0.00, p=0.25; \n",
      "batch 40, cox coeff=0.10, p=0.94, logit coeff=0.10, p=0.94; logit campus_pos coeff=0.00, p=0.33; \n",
      "batch 41, cox coeff=2.21, p=0.07, logit coeff=2.22, p=0.07; logit campus_pos coeff=-0.00, p=0.64; \n",
      "batch 42, cox coeff=1.58, p=0.20, logit coeff=1.58, p=0.19; logit campus_pos coeff=-0.00, p=0.13; \n",
      "batch 43, cox coeff=0.87, p=0.47, logit coeff=0.87, p=0.47; logit campus_pos coeff=0.00, p=0.76; \n",
      "batch 44, cox coeff=0.12, p=0.92, logit coeff=0.12, p=0.92; logit campus_pos coeff=-0.00, p=0.36; \n",
      "batch 45, cox coeff=0.14, p=0.91, logit coeff=0.14, p=0.90; logit campus_pos coeff=0.00, p=0.23; \n",
      "batch 46, cox coeff=-0.02, p=0.99, logit coeff=-0.02, p=0.99; logit campus_pos coeff=0.00, p=0.64; \n",
      "batch 47, cox coeff=-0.27, p=0.83, logit coeff=-0.27, p=0.84; logit campus_pos coeff=0.00, p=0.40; \n",
      "batch 48, cox coeff=-0.39, p=0.75, logit coeff=-0.39, p=0.75; logit campus_pos coeff=-0.00, p=0.53; \n",
      "batch 49, cox coeff=-2.55, p=0.05, logit coeff=-2.55, p=0.04; logit campus_pos coeff=0.00, p=0.39; \n",
      "shape value 1.5\n",
      "batch 0, cox coeff=0.21, p=0.75, logit coeff=0.21, p=0.75; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 1, cox coeff=-0.24, p=0.72, logit coeff=-0.24, p=0.73; logit campus_pos coeff=0.00, p=0.01; \n",
      "batch 2, cox coeff=-0.45, p=0.49, logit coeff=-0.45, p=0.50; logit campus_pos coeff=0.00, p=0.62; \n",
      "batch 3, cox coeff=0.05, p=0.94, logit coeff=0.05, p=0.94; logit campus_pos coeff=0.00, p=0.01; \n",
      "batch 4, cox coeff=-0.60, p=0.36, logit coeff=-0.60, p=0.36; logit campus_pos coeff=0.00, p=0.05; \n",
      "batch 5, cox coeff=0.17, p=0.78, logit coeff=0.17, p=0.79; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 6, cox coeff=1.34, p=0.03, logit coeff=1.35, p=0.03; logit campus_pos coeff=0.00, p=0.06; \n",
      "batch 7, cox coeff=0.38, p=0.55, logit coeff=0.38, p=0.56; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 8, cox coeff=0.78, p=0.21, logit coeff=0.79, p=0.22; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 9, cox coeff=0.92, p=0.15, logit coeff=0.92, p=0.15; logit campus_pos coeff=0.00, p=0.47; \n",
      "batch 10, cox coeff=-0.80, p=0.21, logit coeff=-0.80, p=0.21; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 11, cox coeff=0.10, p=0.88, logit coeff=0.10, p=0.88; logit campus_pos coeff=0.00, p=0.94; \n",
      "batch 12, cox coeff=-0.22, p=0.73, logit coeff=-0.22, p=0.73; logit campus_pos coeff=0.00, p=0.03; \n",
      "batch 13, cox coeff=0.99, p=0.13, logit coeff=0.99, p=0.13; logit campus_pos coeff=0.00, p=0.56; \n",
      "batch 14, cox coeff=1.43, p=0.02, logit coeff=1.43, p=0.02; logit campus_pos coeff=0.00, p=0.35; \n",
      "batch 15, cox coeff=0.67, p=0.29, logit coeff=0.67, p=0.28; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 16, cox coeff=-0.53, p=0.43, logit coeff=-0.53, p=0.42; logit campus_pos coeff=0.00, p=0.01; \n",
      "batch 17, cox coeff=-0.99, p=0.13, logit coeff=-0.99, p=0.13; logit campus_pos coeff=0.00, p=0.01; \n",
      "batch 18, cox coeff=0.31, p=0.62, logit coeff=0.32, p=0.62; logit campus_pos coeff=0.00, p=0.38; \n",
      "batch 19, cox coeff=1.44, p=0.02, logit coeff=1.44, p=0.02; logit campus_pos coeff=0.00, p=0.15; \n",
      "batch 20, cox coeff=1.04, p=0.11, logit coeff=1.04, p=0.12; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 21, cox coeff=0.41, p=0.53, logit coeff=0.41, p=0.54; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 22, cox coeff=0.25, p=0.70, logit coeff=0.25, p=0.69; logit campus_pos coeff=0.00, p=0.04; \n",
      "batch 23, cox coeff=-0.35, p=0.59, logit coeff=-0.35, p=0.58; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 24, cox coeff=0.71, p=0.25, logit coeff=0.71, p=0.25; logit campus_pos coeff=0.00, p=0.03; \n",
      "batch 25, cox coeff=1.31, p=0.03, logit coeff=1.31, p=0.04; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 26, cox coeff=0.49, p=0.44, logit coeff=0.49, p=0.44; logit campus_pos coeff=0.00, p=0.13; \n",
      "batch 27, cox coeff=1.19, p=0.05, logit coeff=1.19, p=0.06; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 28, cox coeff=1.16, p=0.06, logit coeff=1.17, p=0.06; logit campus_pos coeff=0.00, p=0.23; \n",
      "batch 29, cox coeff=1.16, p=0.06, logit coeff=1.16, p=0.06; logit campus_pos coeff=0.00, p=0.02; \n",
      "batch 30, cox coeff=-0.01, p=0.99, logit coeff=-0.01, p=0.99; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 31, cox coeff=0.08, p=0.90, logit coeff=0.08, p=0.90; logit campus_pos coeff=0.00, p=0.03; \n",
      "batch 32, cox coeff=-0.33, p=0.61, logit coeff=-0.34, p=0.60; logit campus_pos coeff=0.00, p=0.01; \n",
      "batch 33, cox coeff=0.57, p=0.37, logit coeff=0.57, p=0.37; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 34, cox coeff=-0.50, p=0.43, logit coeff=-0.51, p=0.42; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 35, cox coeff=-0.03, p=0.97, logit coeff=-0.03, p=0.97; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 36, cox coeff=-0.09, p=0.89, logit coeff=-0.09, p=0.89; logit campus_pos coeff=0.00, p=0.02; \n",
      "batch 37, cox coeff=-0.81, p=0.22, logit coeff=-0.81, p=0.22; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 38, cox coeff=0.37, p=0.56, logit coeff=0.37, p=0.56; logit campus_pos coeff=0.00, p=0.04; \n",
      "batch 39, cox coeff=0.65, p=0.31, logit coeff=0.65, p=0.31; logit campus_pos coeff=0.00, p=0.45; \n",
      "batch 40, cox coeff=0.46, p=0.49, logit coeff=0.46, p=0.47; logit campus_pos coeff=0.00, p=0.86; \n",
      "batch 41, cox coeff=0.47, p=0.46, logit coeff=0.47, p=0.46; logit campus_pos coeff=0.00, p=0.08; \n",
      "batch 42, cox coeff=0.62, p=0.33, logit coeff=0.62, p=0.31; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 43, cox coeff=0.98, p=0.12, logit coeff=0.98, p=0.11; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 44, cox coeff=0.12, p=0.85, logit coeff=0.12, p=0.86; logit campus_pos coeff=0.00, p=0.06; \n",
      "batch 45, cox coeff=0.03, p=0.96, logit coeff=0.03, p=0.96; logit campus_pos coeff=0.00, p=0.23; \n",
      "batch 46, cox coeff=0.91, p=0.14, logit coeff=0.91, p=0.14; logit campus_pos coeff=0.00, p=0.23; \n",
      "batch 47, cox coeff=-0.58, p=0.38, logit coeff=-0.58, p=0.38; logit campus_pos coeff=0.00, p=0.33; \n",
      "batch 48, cox coeff=-1.60, p=0.02, logit coeff=-1.60, p=0.01; logit campus_pos coeff=0.00, p=0.27; \n",
      "batch 49, cox coeff=0.88, p=0.16, logit coeff=0.88, p=0.16; logit campus_pos coeff=0.00, p=0.35; \n",
      "shape value 2\n",
      "batch 0, cox coeff=0.36, p=0.29, logit coeff=0.36, p=0.30; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 1, cox coeff=0.28, p=0.42, logit coeff=0.27, p=0.43; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 2, cox coeff=0.23, p=0.51, logit coeff=0.23, p=0.51; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 3, cox coeff=0.04, p=0.90, logit coeff=0.05, p=0.90; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 4, cox coeff=0.48, p=0.15, logit coeff=0.48, p=0.15; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 5, cox coeff=0.65, p=0.06, logit coeff=0.65, p=0.05; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 6, cox coeff=0.62, p=0.07, logit coeff=0.62, p=0.07; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 7, cox coeff=0.51, p=0.14, logit coeff=0.50, p=0.15; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 8, cox coeff=0.45, p=0.18, logit coeff=0.46, p=0.18; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 9, cox coeff=1.11, p=0.00, logit coeff=1.12, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 10, cox coeff=0.19, p=0.57, logit coeff=0.18, p=0.58; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 11, cox coeff=0.50, p=0.15, logit coeff=0.51, p=0.15; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 12, cox coeff=0.19, p=0.59, logit coeff=0.19, p=0.58; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 13, cox coeff=0.59, p=0.09, logit coeff=0.58, p=0.10; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 14, cox coeff=0.20, p=0.56, logit coeff=0.20, p=0.56; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 15, cox coeff=-0.10, p=0.78, logit coeff=-0.09, p=0.79; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 16, cox coeff=-0.39, p=0.27, logit coeff=-0.40, p=0.26; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 17, cox coeff=-0.21, p=0.54, logit coeff=-0.21, p=0.54; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 18, cox coeff=0.43, p=0.21, logit coeff=0.43, p=0.21; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 19, cox coeff=0.69, p=0.04, logit coeff=0.69, p=0.04; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 20, cox coeff=0.43, p=0.21, logit coeff=0.43, p=0.21; logit campus_pos coeff=0.00, p=0.18; \n",
      "batch 21, cox coeff=0.49, p=0.15, logit coeff=0.48, p=0.16; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 22, cox coeff=0.31, p=0.36, logit coeff=0.31, p=0.35; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 23, cox coeff=0.07, p=0.85, logit coeff=0.06, p=0.86; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 24, cox coeff=0.47, p=0.17, logit coeff=0.47, p=0.18; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 25, cox coeff=0.18, p=0.60, logit coeff=0.18, p=0.61; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 26, cox coeff=0.25, p=0.46, logit coeff=0.25, p=0.47; logit campus_pos coeff=0.00, p=0.01; \n",
      "batch 27, cox coeff=0.68, p=0.04, logit coeff=0.68, p=0.04; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 28, cox coeff=0.78, p=0.02, logit coeff=0.77, p=0.02; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 29, cox coeff=0.66, p=0.05, logit coeff=0.65, p=0.05; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 30, cox coeff=0.51, p=0.15, logit coeff=0.51, p=0.14; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 31, cox coeff=0.59, p=0.08, logit coeff=0.59, p=0.08; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 32, cox coeff=0.60, p=0.08, logit coeff=0.60, p=0.08; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 33, cox coeff=0.72, p=0.04, logit coeff=0.71, p=0.04; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 34, cox coeff=0.07, p=0.85, logit coeff=0.06, p=0.85; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 35, cox coeff=0.09, p=0.80, logit coeff=0.09, p=0.79; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 36, cox coeff=1.15, p=0.00, logit coeff=1.14, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 37, cox coeff=-0.15, p=0.67, logit coeff=-0.15, p=0.67; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 38, cox coeff=0.35, p=0.31, logit coeff=0.35, p=0.32; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 39, cox coeff=-0.05, p=0.89, logit coeff=-0.05, p=0.89; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 40, cox coeff=0.60, p=0.08, logit coeff=0.61, p=0.08; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 41, cox coeff=0.27, p=0.44, logit coeff=0.27, p=0.44; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 42, cox coeff=0.42, p=0.22, logit coeff=0.42, p=0.21; logit campus_pos coeff=0.00, p=0.02; \n",
      "batch 43, cox coeff=0.05, p=0.88, logit coeff=0.05, p=0.88; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 44, cox coeff=0.14, p=0.68, logit coeff=0.15, p=0.66; logit campus_pos coeff=0.00, p=0.01; \n",
      "batch 45, cox coeff=-0.18, p=0.61, logit coeff=-0.17, p=0.61; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 46, cox coeff=-0.31, p=0.37, logit coeff=-0.30, p=0.37; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 47, cox coeff=0.77, p=0.02, logit coeff=0.76, p=0.03; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 48, cox coeff=0.05, p=0.90, logit coeff=0.04, p=0.90; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 49, cox coeff=-0.29, p=0.40, logit coeff=-0.29, p=0.40; logit campus_pos coeff=0.00, p=0.00; \n"
     ]
    }
   ],
   "source": [
    "# sampling class positivity from a distributioun with constant mean over time\n",
    "\n",
    "logit_res = defaultdict(list)\n",
    "cox_res = defaultdict(list)\n",
    "classpos_time_corr = defaultdict(list)\n",
    "\n",
    "for shape_val in [0.5, 1.0, 1.5, 2]:\n",
    "    print(\"shape value\", shape_val)\n",
    "    for batch in range(50):\n",
    "        sim_data = pd.read_csv(\n",
    "            # f\"../data/simulation/SampledFeature=cp_OutcomeGenenerator=cox_shape={shape_val}_batch={batch}.csv\",\n",
    "            # f\"../data/simulation/SampledFeature=cp_OutcomeGenenerator=cox_scale=0.001_shape={shape_val}_batch={batch}.csv\",\n",
    "            f\"../data/simulation/SampledFeature=cp_sampling=aggregate_OutcomeGenenerator=cox_scale=0.001_shape={shape_val}_batch={batch}.csv\",\n",
    "            index_col=0\n",
    "        )\n",
    "\n",
    "        ctv = sim.fit_cox(sim_data)\n",
    "        ctv_lower, ctv_upper = ctv.confidence_intervals_.values[0]\n",
    "        cox_res[shape_val].append(\n",
    "            {\"coef\": ctv.params_[\"class_positivity\"], \n",
    "            \"p-value\": ctv._compute_p_values()[0],\n",
    "            \"coverage\": TRUE_COEF >= ctv_lower and TRUE_COEF <= ctv_upper}\n",
    "        )\n",
    "\n",
    "        sim_data[\"week_idx\"] = sim_data[\"day_idx\"] // 7\n",
    "        logit = sim.fit_logistic(sim_data)\n",
    "        logit_lower, logit_upper = logit.conf_int().loc[\"class_positivity\"].values\n",
    "        logit_res[shape_val].append(\n",
    "            {\"coef\": logit._results.params[-2],\n",
    "            \"p-value\": logit.pvalues[\"class_positivity\"],\n",
    "            \"coverage\": TRUE_COEF >= logit_lower and TRUE_COEF <= logit_upper}\n",
    "        )\n",
    "\n",
    "        corr = sim_data.corr()\n",
    "        classpos_time_corr[shape_val].append(corr['class_positivity']['day_idx'])\n",
    "\n",
    "        print(\n",
    "            f\"batch {batch}, cox coeff={ctv.params_['class_positivity']:.2f}, p={ctv._compute_p_values()[0]:.2f}, logit coeff={logit._results.params[-2]:.2f}, p={logit.pvalues['class_positivity']:.2f}; \"\n",
    "            f\"logit campus_pos coeff={logit._results.params[-1]:.2f}, p={logit.pvalues['campus_positivity']:.2f}; \")\n",
    "        # print(\n",
    "        #     f\"class_pos and time correlation={corr['class_positivity']['day_idx']:.2f}; \"\n",
    "        #     f\"class_pos and campus_pos correlation={corr['class_positivity']['campus_positivity']:.2f}; \"\n",
    "        # )\n",
    "\n",
    "        setting = \"SampledFeature=cp_sampling=aggregate_OutcomeGenenerator=cox_scale=0.001\"\n",
    "        with open (\"logit_res_\" + setting + \".pkl\", \"wb\") as f:\n",
    "            pickle.dump(logit_res, f)\n",
    "        with open (\"cox_res_\" + setting + \".pkl\", \"wb\") as f:\n",
    "            pickle.dump(cox_res, f)\n",
    "        with open(\"classpos_time_corr\" + setting + \".pkl\", \"wb\") as f:\n",
    "            pickle.dump(classpos_time_corr, f)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape value 0.5\n",
      "batch 0, cox coeff=2.64, p=0.31, logit coeff=2.56, p=0.27; logit campus_pos coeff=-0.00, p=0.49; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.313. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1, cox coeff=2.10, p=0.53, logit coeff=0.80, p=0.77; logit campus_pos coeff=-0.00, p=0.47; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GR have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GR'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GR'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GR completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.308. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2, cox coeff=1.06, p=0.70, logit coeff=0.36, p=0.88; logit campus_pos coeff=0.00, p=0.81; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_VM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_VM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_VM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_VM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.164. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3, cox coeff=3.40, p=0.25, logit coeff=2.37, p=0.43; logit campus_pos coeff=0.00, p=0.95; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GR have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GR'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GR'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GR completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.308. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 4, cox coeff=5.64, p=0.06, logit coeff=4.36, p=0.12; logit campus_pos coeff=-0.00, p=0.41; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_VM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_VM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_VM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_VM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.171. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 5, cox coeff=-8.02, p=0.06, logit coeff=-7.95, p=0.02; logit campus_pos coeff=-0.01, p=0.06; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_VM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_VM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_VM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_VM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.171. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 6, cox coeff=-2.96, p=0.42, logit coeff=-2.79, p=0.26; logit campus_pos coeff=0.00, p=0.74; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.311. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 7, cox coeff=0.43, p=0.90, logit coeff=-0.40, p=0.89; logit campus_pos coeff=-0.00, p=0.51; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.311. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 8, cox coeff=-2.64, p=0.50, logit coeff=-2.95, p=0.53; logit campus_pos coeff=0.00, p=0.82; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_VM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_VM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_VM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_VM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.171. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 9, cox coeff=2.04, p=0.56, logit coeff=0.74, p=0.81; logit campus_pos coeff=-0.01, p=0.04; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.166. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 10, cox coeff=0.54, p=0.86, logit coeff=-2.03, p=0.30; logit campus_pos coeff=-0.01, p=0.13; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_UG_A have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_UG_A'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_UG_A'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_UG_A completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.208. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 11, cox coeff=0.32, p=0.91, logit coeff=-0.70, p=0.85; logit campus_pos coeff=-0.01, p=0.01; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_VM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_VM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_VM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_VM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.164. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 12, cox coeff=1.29, p=0.71, logit coeff=0.24, p=0.93; logit campus_pos coeff=-0.00, p=0.52; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_UG_A have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_UG_A'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_UG_A'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_UG_A completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.208. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 13, cox coeff=-0.33, p=0.92, logit coeff=-0.83, p=0.80; logit campus_pos coeff=0.00, p=0.44; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_UG_A have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_UG_A'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_UG_A'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_UG_A completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.264. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 14, cox coeff=1.30, p=0.72, logit coeff=-1.49, p=0.64; logit campus_pos coeff=-0.00, p=0.37; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GR have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GR'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GR'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GR completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.308. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 15, cox coeff=3.84, p=0.23, logit coeff=2.44, p=0.42; logit campus_pos coeff=-0.00, p=0.72; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_UG_A have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_UG_A'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_UG_A'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_UG_A completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.269. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 16, cox coeff=-3.77, p=0.39, logit coeff=-5.17, p=0.23; logit campus_pos coeff=-0.00, p=0.45; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.268. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 17, cox coeff=-6.11, p=0.15, logit coeff=-6.36, p=0.18; logit campus_pos coeff=-0.01, p=0.14; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_LA have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_LA'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_LA'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_LA completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.331. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 18, cox coeff=2.01, p=0.52, logit coeff=0.59, p=0.84; logit campus_pos coeff=-0.00, p=0.19; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.173. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 19, cox coeff=0.76, p=0.80, logit coeff=0.79, p=0.79; logit campus_pos coeff=-0.00, p=0.54; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_UG_A have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_UG_A'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_UG_A'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_UG_A completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.208. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 20, cox coeff=-6.47, p=0.09, logit coeff=-7.21, p=0.02; logit campus_pos coeff=-0.01, p=0.13; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.243. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 21, cox coeff=-1.22, p=0.68, logit coeff=-1.14, p=0.63; logit campus_pos coeff=-0.01, p=0.08; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_VM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_VM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_VM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_VM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.171. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 22, cox coeff=2.89, p=0.39, logit coeff=2.57, p=0.41; logit campus_pos coeff=-0.02, p=0.00; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_VM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_VM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_VM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_VM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.164. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 23, cox coeff=-1.94, p=0.56, logit coeff=-3.12, p=0.40; logit campus_pos coeff=-0.00, p=0.40; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GR have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GR'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GR'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GR completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.369. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 24, cox coeff=-2.23, p=0.50, logit coeff=-2.31, p=0.48; logit campus_pos coeff=-0.00, p=0.69; \n",
      "batch 25, cox coeff=-0.05, p=0.99, logit coeff=-1.15, p=0.67; logit campus_pos coeff=-0.00, p=0.52; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.351. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 26, cox coeff=4.32, p=0.14, logit coeff=4.61, p=0.16; logit campus_pos coeff=-0.00, p=0.66; \n",
      "batch 27, cox coeff=1.71, p=0.58, logit coeff=-0.28, p=0.91; logit campus_pos coeff=-0.01, p=0.02; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_UG_A have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_UG_A'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_UG_A'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_UG_A completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.208. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 28, cox coeff=6.15, p=0.01, logit coeff=4.50, p=0.05; logit campus_pos coeff=-0.01, p=0.01; \n",
      "batch 29, cox coeff=0.02, p=0.99, logit coeff=-0.93, p=0.72; logit campus_pos coeff=0.00, p=0.96; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.173. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 30, cox coeff=1.61, p=0.56, logit coeff=-0.45, p=0.86; logit campus_pos coeff=0.00, p=0.84; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GR have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GR'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GR'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GR completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.308. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 31, cox coeff=-4.42, p=0.23, logit coeff=-4.62, p=0.24; logit campus_pos coeff=0.00, p=0.46; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.372. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 32, cox coeff=4.14, p=0.15, logit coeff=3.28, p=0.25; logit campus_pos coeff=-0.01, p=0.00; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_LA have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_LA'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_LA'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_LA completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.261. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 33, cox coeff=2.14, p=0.41, logit coeff=1.78, p=0.47; logit campus_pos coeff=-0.02, p=0.00; \n",
      "batch 34, cox coeff=-3.47, p=0.33, logit coeff=-4.22, p=0.27; logit campus_pos coeff=-0.01, p=0.05; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_VM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_VM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_VM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_VM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.171. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 35, cox coeff=-1.42, p=0.62, logit coeff=-1.22, p=0.68; logit campus_pos coeff=-0.00, p=0.38; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_VM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_VM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_VM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_VM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.164. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 36, cox coeff=-1.61, p=0.67, logit coeff=-2.66, p=0.43; logit campus_pos coeff=-0.00, p=0.97; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.309. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 37, cox coeff=-3.17, p=0.41, logit coeff=-4.09, p=0.26; logit campus_pos coeff=-0.00, p=0.28; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.270. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 38, cox coeff=4.06, p=0.22, logit coeff=3.14, p=0.38; logit campus_pos coeff=-0.01, p=0.07; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_UG_A have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_UG_A'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_UG_A'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_UG_A completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.208. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 39, cox coeff=0.30, p=0.93, logit coeff=-2.18, p=0.42; logit campus_pos coeff=-0.00, p=0.40; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.270. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 40, cox coeff=-2.67, p=0.41, logit coeff=-2.85, p=0.42; logit campus_pos coeff=0.01, p=0.02; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_VM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_VM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_VM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_VM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.164. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 41, cox coeff=-1.59, p=0.68, logit coeff=-2.40, p=0.54; logit campus_pos coeff=-0.01, p=0.28; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.352. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 42, cox coeff=-5.04, p=0.19, logit coeff=-5.19, p=0.15; logit campus_pos coeff=-0.01, p=0.07; \n",
      "batch 43, cox coeff=1.48, p=0.63, logit coeff=1.54, p=0.63; logit campus_pos coeff=-0.00, p=0.84; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_LA have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_LA'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_LA'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_LA completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.261. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 44, cox coeff=-1.98, p=0.59, logit coeff=-4.00, p=0.32; logit campus_pos coeff=-0.00, p=0.49; \n",
      "batch 45, cox coeff=5.20, p=0.05, logit coeff=5.10, p=0.01; logit campus_pos coeff=-0.00, p=0.24; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.243. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 46, cox coeff=2.39, p=0.37, logit coeff=1.30, p=0.65; logit campus_pos coeff=-0.01, p=0.13; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_UG_A have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_UG_A'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_UG_A'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_UG_A completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.264. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 47, cox coeff=-3.18, p=0.37, logit coeff=-4.06, p=0.29; logit campus_pos coeff=-0.00, p=0.74; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.243. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 48, cox coeff=1.30, p=0.68, logit coeff=0.49, p=0.89; logit campus_pos coeff=-0.01, p=0.06; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_GM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_GM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_GM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_GM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.270. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 49, cox coeff=2.56, p=0.38, logit coeff=1.50, p=0.59; logit campus_pos coeff=-0.00, p=0.44; \n",
      "shape value 1.0\n",
      "batch 0, cox coeff=1.85, p=0.13, logit coeff=1.99, p=0.07; logit campus_pos coeff=-0.00, p=0.42; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_VM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_VM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_VM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_VM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.168. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1, cox coeff=1.49, p=0.25, logit coeff=1.56, p=0.16; logit campus_pos coeff=0.00, p=0.79; \n",
      "batch 2, cox coeff=0.79, p=0.53, logit coeff=0.73, p=0.51; logit campus_pos coeff=-0.00, p=0.82; \n",
      "batch 3, cox coeff=-3.53, p=0.01, logit coeff=-2.98, p=0.02; logit campus_pos coeff=-0.00, p=0.28; \n",
      "batch 4, cox coeff=0.80, p=0.55, logit coeff=0.41, p=0.76; logit campus_pos coeff=0.00, p=0.13; \n",
      "batch 5, cox coeff=-1.03, p=0.46, logit coeff=-1.44, p=0.31; logit campus_pos coeff=0.00, p=0.22; \n",
      "batch 6, cox coeff=0.94, p=0.44, logit coeff=1.13, p=0.35; logit campus_pos coeff=0.00, p=0.73; \n",
      "batch 7, cox coeff=-0.71, p=0.58, logit coeff=-0.14, p=0.92; logit campus_pos coeff=-0.00, p=0.71; \n",
      "batch 8, cox coeff=0.48, p=0.72, logit coeff=0.25, p=0.84; logit campus_pos coeff=-0.00, p=0.54; \n",
      "batch 9, cox coeff=-0.13, p=0.92, logit coeff=-0.26, p=0.83; logit campus_pos coeff=0.00, p=0.67; \n",
      "batch 10, cox coeff=0.85, p=0.49, logit coeff=0.85, p=0.49; logit campus_pos coeff=-0.00, p=1.00; \n",
      "batch 11, cox coeff=-0.02, p=0.99, logit coeff=0.02, p=0.99; logit campus_pos coeff=-0.00, p=0.72; \n",
      "batch 12, cox coeff=-0.75, p=0.57, logit coeff=-0.63, p=0.60; logit campus_pos coeff=-0.00, p=0.35; \n",
      "batch 13, cox coeff=0.69, p=0.59, logit coeff=1.05, p=0.41; logit campus_pos coeff=0.00, p=0.15; \n",
      "batch 14, cox coeff=0.06, p=0.97, logit coeff=-0.12, p=0.93; logit campus_pos coeff=0.00, p=0.55; \n",
      "batch 15, cox coeff=0.49, p=0.70, logit coeff=-0.00, p=1.00; logit campus_pos coeff=0.00, p=0.41; \n",
      "batch 16, cox coeff=0.41, p=0.76, logit coeff=0.42, p=0.73; logit campus_pos coeff=0.00, p=0.07; \n",
      "batch 17, cox coeff=0.29, p=0.84, logit coeff=0.26, p=0.85; logit campus_pos coeff=-0.00, p=0.26; \n",
      "batch 18, cox coeff=1.63, p=0.18, logit coeff=1.67, p=0.17; logit campus_pos coeff=0.00, p=0.62; \n",
      "batch 19, cox coeff=0.63, p=0.60, logit coeff=0.83, p=0.50; logit campus_pos coeff=-0.00, p=0.75; \n",
      "batch 20, cox coeff=-2.81, p=0.06, logit coeff=-2.30, p=0.11; logit campus_pos coeff=-0.00, p=0.80; \n",
      "batch 21, cox coeff=-0.10, p=0.94, logit coeff=0.35, p=0.79; logit campus_pos coeff=-0.00, p=0.33; \n",
      "batch 22, cox coeff=1.53, p=0.19, logit coeff=1.88, p=0.10; logit campus_pos coeff=-0.00, p=0.07; \n",
      "batch 23, cox coeff=-0.48, p=0.72, logit coeff=-0.44, p=0.75; logit campus_pos coeff=-0.00, p=0.20; \n",
      "batch 24, cox coeff=-0.99, p=0.44, logit coeff=-0.89, p=0.50; logit campus_pos coeff=0.00, p=0.71; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\utils\\__init__.py:1122: ConvergenceWarning: Column academic_career_VM have very low variance when conditioned on death event present or not. This may harm convergence. This could be a form of 'complete separation'. For example, try the following code:\n",
      "\n",
      ">>> events = df['infected_on_this_day'].astype(bool)\n",
      ">>> print(df.loc[events, 'academic_career_VM'].var())\n",
      ">>> print(df.loc[~events, 'academic_career_VM'].var())\n",
      "\n",
      "A very low variance means that the column academic_career_VM completely determines whether a subject dies or not. See https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression.\n",
      "\n",
      "  warnings.warn(dedent(warning_text), ConvergenceWarning)\n",
      "C:\\Users\\yz685\\AppData\\Roaming\\Python\\Python37\\site-packages\\lifelines\\fitters\\cox_time_varying_fitter.py:491: ConvergenceWarning: Newton-Raphson convergence completed but norm(delta) is still high, 0.168. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 25, cox coeff=-0.73, p=0.58, logit coeff=-0.96, p=0.44; logit campus_pos coeff=0.00, p=0.36; \n",
      "batch 26, cox coeff=0.93, p=0.45, logit coeff=1.03, p=0.35; logit campus_pos coeff=-0.00, p=0.62; \n",
      "batch 27, cox coeff=1.02, p=0.42, logit coeff=0.62, p=0.61; logit campus_pos coeff=0.00, p=0.48; \n",
      "batch 28, cox coeff=2.39, p=0.07, logit coeff=1.97, p=0.14; logit campus_pos coeff=0.00, p=0.76; \n",
      "batch 29, cox coeff=0.77, p=0.53, logit coeff=1.08, p=0.40; logit campus_pos coeff=0.00, p=0.02; \n",
      "batch 30, cox coeff=2.04, p=0.12, logit coeff=1.61, p=0.18; logit campus_pos coeff=0.00, p=0.93; \n",
      "batch 31, cox coeff=-0.93, p=0.49, logit coeff=-0.81, p=0.56; logit campus_pos coeff=0.00, p=0.98; \n",
      "batch 32, cox coeff=1.09, p=0.40, logit coeff=0.81, p=0.52; logit campus_pos coeff=-0.00, p=0.22; \n",
      "batch 33, cox coeff=0.70, p=0.58, logit coeff=0.73, p=0.59; logit campus_pos coeff=0.00, p=0.74; \n",
      "batch 34, cox coeff=0.45, p=0.73, logit coeff=0.23, p=0.85; logit campus_pos coeff=-0.00, p=0.81; \n",
      "batch 35, cox coeff=-1.22, p=0.38, logit coeff=-1.24, p=0.35; logit campus_pos coeff=-0.00, p=0.05; \n",
      "batch 36, cox coeff=1.08, p=0.40, logit coeff=1.29, p=0.34; logit campus_pos coeff=-0.00, p=0.60; \n",
      "batch 37, cox coeff=-0.17, p=0.90, logit coeff=-0.25, p=0.85; logit campus_pos coeff=0.00, p=0.47; \n",
      "batch 38, cox coeff=-0.22, p=0.87, logit coeff=0.01, p=1.00; logit campus_pos coeff=-0.00, p=0.80; \n",
      "batch 39, cox coeff=-0.06, p=0.96, logit coeff=0.04, p=0.97; logit campus_pos coeff=0.00, p=0.15; \n",
      "batch 40, cox coeff=0.56, p=0.68, logit coeff=0.03, p=0.98; logit campus_pos coeff=0.00, p=0.07; \n",
      "batch 41, cox coeff=1.31, p=0.28, logit coeff=1.87, p=0.10; logit campus_pos coeff=-0.00, p=0.68; \n",
      "batch 42, cox coeff=-0.59, p=0.65, logit coeff=-0.27, p=0.83; logit campus_pos coeff=0.00, p=0.33; \n",
      "batch 43, cox coeff=0.18, p=0.89, logit coeff=0.45, p=0.71; logit campus_pos coeff=0.00, p=0.33; \n",
      "batch 44, cox coeff=-1.25, p=0.34, logit coeff=-0.98, p=0.47; logit campus_pos coeff=-0.00, p=0.55; \n",
      "batch 45, cox coeff=1.23, p=0.34, logit coeff=0.06, p=0.96; logit campus_pos coeff=-0.00, p=0.77; \n",
      "batch 46, cox coeff=2.40, p=0.03, logit coeff=2.68, p=0.01; logit campus_pos coeff=0.00, p=0.52; \n",
      "batch 47, cox coeff=0.71, p=0.57, logit coeff=0.87, p=0.49; logit campus_pos coeff=0.00, p=0.71; \n",
      "batch 48, cox coeff=0.52, p=0.66, logit coeff=0.81, p=0.52; logit campus_pos coeff=0.00, p=0.08; \n",
      "batch 49, cox coeff=0.94, p=0.43, logit coeff=0.95, p=0.42; logit campus_pos coeff=-0.00, p=0.31; \n",
      "shape value 1.5\n",
      "batch 0, cox coeff=0.34, p=0.57, logit coeff=0.87, p=0.13; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 1, cox coeff=0.31, p=0.59, logit coeff=0.83, p=0.15; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 2, cox coeff=0.69, p=0.25, logit coeff=1.21, p=0.04; logit campus_pos coeff=0.00, p=0.01; \n",
      "batch 3, cox coeff=0.33, p=0.58, logit coeff=0.97, p=0.11; logit campus_pos coeff=0.00, p=0.04; \n",
      "batch 4, cox coeff=-0.47, p=0.45, logit coeff=0.03, p=0.95; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 5, cox coeff=0.55, p=0.35, logit coeff=1.33, p=0.02; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 6, cox coeff=0.41, p=0.48, logit coeff=1.09, p=0.05; logit campus_pos coeff=0.00, p=0.08; \n",
      "batch 7, cox coeff=0.08, p=0.90, logit coeff=0.38, p=0.52; logit campus_pos coeff=0.00, p=0.02; \n",
      "batch 8, cox coeff=-0.63, p=0.31, logit coeff=0.03, p=0.96; logit campus_pos coeff=0.00, p=0.03; \n",
      "batch 9, cox coeff=0.59, p=0.33, logit coeff=1.06, p=0.07; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 10, cox coeff=0.82, p=0.17, logit coeff=1.16, p=0.04; logit campus_pos coeff=0.00, p=0.17; \n",
      "batch 11, cox coeff=0.83, p=0.16, logit coeff=1.46, p=0.01; logit campus_pos coeff=0.00, p=0.43; \n",
      "batch 12, cox coeff=-0.24, p=0.69, logit coeff=0.51, p=0.40; logit campus_pos coeff=0.00, p=0.07; \n",
      "batch 13, cox coeff=-0.14, p=0.82, logit coeff=0.51, p=0.41; logit campus_pos coeff=0.00, p=0.01; \n",
      "batch 14, cox coeff=1.20, p=0.04, logit coeff=1.79, p=0.00; logit campus_pos coeff=0.00, p=0.01; \n",
      "batch 15, cox coeff=0.13, p=0.82, logit coeff=0.52, p=0.37; logit campus_pos coeff=0.00, p=0.02; \n",
      "batch 16, cox coeff=1.21, p=0.04, logit coeff=1.73, p=0.00; logit campus_pos coeff=0.00, p=0.01; \n",
      "batch 17, cox coeff=-0.75, p=0.23, logit coeff=-0.17, p=0.78; logit campus_pos coeff=0.00, p=0.07; \n",
      "batch 18, cox coeff=0.15, p=0.80, logit coeff=0.49, p=0.39; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 19, cox coeff=-0.24, p=0.68, logit coeff=0.55, p=0.34; logit campus_pos coeff=0.00, p=0.08; \n",
      "batch 20, cox coeff=-0.68, p=0.27, logit coeff=-0.05, p=0.94; logit campus_pos coeff=0.00, p=0.02; \n",
      "batch 21, cox coeff=0.21, p=0.72, logit coeff=0.83, p=0.20; logit campus_pos coeff=0.00, p=0.08; \n",
      "batch 22, cox coeff=0.50, p=0.39, logit coeff=1.23, p=0.03; logit campus_pos coeff=0.00, p=0.27; \n",
      "batch 23, cox coeff=-0.92, p=0.14, logit coeff=-0.44, p=0.48; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 24, cox coeff=-0.02, p=0.97, logit coeff=0.67, p=0.27; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 25, cox coeff=-0.40, p=0.50, logit coeff=0.13, p=0.83; logit campus_pos coeff=0.00, p=0.03; \n",
      "batch 26, cox coeff=-0.69, p=0.26, logit coeff=-0.28, p=0.63; logit campus_pos coeff=0.00, p=0.20; \n",
      "batch 27, cox coeff=0.43, p=0.45, logit coeff=0.97, p=0.08; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 28, cox coeff=0.21, p=0.73, logit coeff=0.66, p=0.25; logit campus_pos coeff=0.00, p=0.18; \n",
      "batch 29, cox coeff=0.05, p=0.94, logit coeff=0.71, p=0.23; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 30, cox coeff=1.08, p=0.07, logit coeff=1.59, p=0.01; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 31, cox coeff=-0.01, p=0.99, logit coeff=0.38, p=0.52; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 32, cox coeff=1.26, p=0.03, logit coeff=1.85, p=0.00; logit campus_pos coeff=0.00, p=0.04; \n",
      "batch 33, cox coeff=0.03, p=0.96, logit coeff=0.59, p=0.32; logit campus_pos coeff=0.00, p=0.48; \n",
      "batch 34, cox coeff=0.02, p=0.97, logit coeff=0.45, p=0.46; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 35, cox coeff=0.19, p=0.74, logit coeff=0.61, p=0.28; logit campus_pos coeff=0.00, p=0.69; \n",
      "batch 36, cox coeff=-0.14, p=0.82, logit coeff=0.50, p=0.42; logit campus_pos coeff=0.00, p=0.05; \n",
      "batch 37, cox coeff=0.84, p=0.15, logit coeff=1.56, p=0.01; logit campus_pos coeff=0.00, p=0.18; \n",
      "batch 38, cox coeff=-0.11, p=0.86, logit coeff=0.25, p=0.69; logit campus_pos coeff=0.00, p=0.02; \n",
      "batch 39, cox coeff=-0.20, p=0.74, logit coeff=0.51, p=0.40; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 40, cox coeff=0.12, p=0.85, logit coeff=0.79, p=0.17; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 41, cox coeff=0.82, p=0.17, logit coeff=1.47, p=0.02; logit campus_pos coeff=0.00, p=0.01; \n",
      "batch 42, cox coeff=0.62, p=0.30, logit coeff=1.30, p=0.02; logit campus_pos coeff=0.00, p=0.01; \n",
      "batch 43, cox coeff=0.09, p=0.87, logit coeff=0.59, p=0.33; logit campus_pos coeff=0.00, p=0.52; \n",
      "batch 44, cox coeff=0.03, p=0.97, logit coeff=0.50, p=0.43; logit campus_pos coeff=0.00, p=0.17; \n",
      "batch 45, cox coeff=-0.58, p=0.35, logit coeff=0.02, p=0.98; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 46, cox coeff=-0.25, p=0.66, logit coeff=0.59, p=0.31; logit campus_pos coeff=0.00, p=0.01; \n",
      "batch 47, cox coeff=-0.20, p=0.74, logit coeff=0.62, p=0.31; logit campus_pos coeff=0.00, p=0.06; \n",
      "batch 48, cox coeff=0.11, p=0.85, logit coeff=0.64, p=0.29; logit campus_pos coeff=0.00, p=0.10; \n",
      "batch 49, cox coeff=-0.45, p=0.47, logit coeff=0.18, p=0.77; logit campus_pos coeff=0.00, p=0.04; \n",
      "shape value 2\n",
      "batch 0, cox coeff=-0.16, p=0.59, logit coeff=0.82, p=0.01; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 1, cox coeff=0.16, p=0.60, logit coeff=1.03, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 2, cox coeff=0.28, p=0.35, logit coeff=1.18, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 3, cox coeff=0.43, p=0.16, logit coeff=1.44, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 4, cox coeff=0.13, p=0.67, logit coeff=0.97, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 5, cox coeff=0.74, p=0.01, logit coeff=1.69, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 6, cox coeff=0.05, p=0.86, logit coeff=1.02, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 7, cox coeff=-0.13, p=0.68, logit coeff=0.85, p=0.01; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 8, cox coeff=0.31, p=0.31, logit coeff=1.21, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 9, cox coeff=0.11, p=0.73, logit coeff=1.06, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 10, cox coeff=0.11, p=0.71, logit coeff=1.08, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 11, cox coeff=0.59, p=0.05, logit coeff=1.57, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 12, cox coeff=0.58, p=0.05, logit coeff=1.56, p=0.00; logit campus_pos coeff=0.00, p=0.01; \n",
      "batch 13, cox coeff=-0.34, p=0.28, logit coeff=0.50, p=0.11; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 14, cox coeff=0.40, p=0.19, logit coeff=1.23, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 15, cox coeff=-0.05, p=0.87, logit coeff=0.89, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 16, cox coeff=0.21, p=0.50, logit coeff=1.13, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 17, cox coeff=0.00, p=1.00, logit coeff=0.79, p=0.01; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 18, cox coeff=0.14, p=0.65, logit coeff=1.10, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 19, cox coeff=0.74, p=0.01, logit coeff=1.55, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 20, cox coeff=0.31, p=0.31, logit coeff=1.28, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 21, cox coeff=0.71, p=0.02, logit coeff=1.70, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 22, cox coeff=0.04, p=0.88, logit coeff=1.06, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 23, cox coeff=0.46, p=0.13, logit coeff=1.24, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 24, cox coeff=0.50, p=0.10, logit coeff=1.55, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 25, cox coeff=0.23, p=0.44, logit coeff=1.18, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 26, cox coeff=0.68, p=0.02, logit coeff=1.61, p=0.00; logit campus_pos coeff=0.00, p=0.01; \n",
      "batch 27, cox coeff=-0.05, p=0.87, logit coeff=0.86, p=0.01; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 28, cox coeff=0.51, p=0.09, logit coeff=1.31, p=0.00; logit campus_pos coeff=0.00, p=0.20; \n",
      "batch 29, cox coeff=0.34, p=0.26, logit coeff=1.30, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 30, cox coeff=0.06, p=0.85, logit coeff=0.90, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 31, cox coeff=0.13, p=0.67, logit coeff=1.23, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 32, cox coeff=0.35, p=0.24, logit coeff=1.20, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 33, cox coeff=0.15, p=0.62, logit coeff=1.22, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 34, cox coeff=0.60, p=0.05, logit coeff=1.52, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 35, cox coeff=0.05, p=0.86, logit coeff=1.00, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 36, cox coeff=0.52, p=0.09, logit coeff=1.42, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 37, cox coeff=0.46, p=0.12, logit coeff=1.46, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 38, cox coeff=0.66, p=0.03, logit coeff=1.56, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 39, cox coeff=0.69, p=0.02, logit coeff=1.61, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 40, cox coeff=1.09, p=0.00, logit coeff=2.05, p=0.00; logit campus_pos coeff=0.00, p=0.01; \n",
      "batch 41, cox coeff=0.58, p=0.06, logit coeff=1.44, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 42, cox coeff=-0.07, p=0.82, logit coeff=0.90, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 43, cox coeff=0.49, p=0.11, logit coeff=1.34, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 44, cox coeff=0.03, p=0.93, logit coeff=0.98, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 45, cox coeff=0.29, p=0.32, logit coeff=1.25, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 46, cox coeff=0.54, p=0.07, logit coeff=1.51, p=0.00; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 47, cox coeff=-0.24, p=0.43, logit coeff=0.63, p=0.04; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 48, cox coeff=0.07, p=0.82, logit coeff=0.90, p=0.01; logit campus_pos coeff=0.00, p=0.00; \n",
      "batch 49, cox coeff=-0.04, p=0.89, logit coeff=0.86, p=0.01; logit campus_pos coeff=0.00, p=0.00; \n"
     ]
    }
   ],
   "source": [
    "# sampling class positivity from a distribution with a different mean every day\n",
    "\n",
    "logit_res = defaultdict(list)\n",
    "cox_res = defaultdict(list)\n",
    "classpos_time_corr = defaultdict(list)\n",
    "\n",
    "for shape_val in [0.5, 1.0, 1.5, 2]:\n",
    "    print(\"shape value\", shape_val)\n",
    "    for batch in range(50):\n",
    "        sim_data = pd.read_csv(\n",
    "            # f\"../data/simulation/SampledFeature=cp_OutcomeGenenerator=cox_shape={shape_val}_batch={batch}.csv\",\n",
    "            f\"../data/simulation/SampledFeature=cp_OutcomeGenenerator=cox_scale=0.001_shape={shape_val}_batch={batch}.csv\",\n",
    "            # f\"../data/simulation/SampledFeature=cp_sampling=aggregate_OutcomeGenenerator=cox_scale=0.001_shape={shape_val}_batch={batch}.csv\",\n",
    "            index_col=0\n",
    "        )\n",
    "\n",
    "        ctv = sim.fit_cox(sim_data)\n",
    "        ctv_lower, ctv_upper = ctv.confidence_intervals_.values[0]\n",
    "        cox_res[shape_val].append(\n",
    "            {\"coef\": ctv.params_[\"class_positivity\"], \n",
    "            \"p-value\": ctv._compute_p_values()[0],\n",
    "            \"coverage\": TRUE_COEF >= ctv_lower and TRUE_COEF <= ctv_upper}\n",
    "        )\n",
    "\n",
    "        sim_data[\"week_idx\"] = sim_data[\"day_idx\"] // 7\n",
    "        logit = sim.fit_logistic(sim_data)\n",
    "        logit_lower, logit_upper = logit.conf_int().loc[\"class_positivity\"].values\n",
    "        logit_res[shape_val].append(\n",
    "            {\"coef\": logit._results.params[-2],\n",
    "            \"p-value\": logit.pvalues[\"class_positivity\"],\n",
    "            \"coverage\": TRUE_COEF >= logit_lower and TRUE_COEF <= logit_upper}\n",
    "        )\n",
    "\n",
    "        corr = sim_data.corr()\n",
    "        classpos_time_corr[shape_val].append(corr['class_positivity']['day_idx'])\n",
    "\n",
    "        print(\n",
    "            f\"batch {batch}, cox coeff={ctv.params_['class_positivity']:.2f}, p={ctv._compute_p_values()[0]:.2f}, logit coeff={logit._results.params[-2]:.2f}, p={logit.pvalues['class_positivity']:.2f}; \"\n",
    "            f\"logit campus_pos coeff={logit._results.params[-1]:.2f}, p={logit.pvalues['campus_positivity']:.2f}; \")\n",
    "\n",
    "        setting = \"SampledFeature=cp_sampling=byday_OutcomeGenenerator=cox_scale=0.001\"\n",
    "        with open (\"logit_res_\" + setting + \".pkl\", \"wb\") as f:\n",
    "            pickle.dump(logit_res, f)\n",
    "        with open (\"cox_res_\" + setting + \".pkl\", \"wb\") as f:\n",
    "            pickle.dump(cox_res, f)\n",
    "        with open(\"classpos_time_corr\" + setting + \".pkl\", \"wb\") as f:\n",
    "            pickle.dump(classpos_time_corr, f)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"aggregate\": \"SampledFeature=cp_sampling=aggregate_OutcomeGenenerator=cox_scale=0.001\",\n",
    "    \"byday\": \"SampledFeature=cp_sampling=byday_OutcomeGenenerator=cox_scale=0.001\"\n",
    "}\n",
    "\n",
    "loaded_logit_res = {}\n",
    "loaded_cox_res = {}\n",
    "loaded_classpos_time_corr = {}\n",
    "\n",
    "for s in [\"aggregate\", \"byday\"]:\n",
    "\n",
    "    with open (\"logit_res_\" + settings[s] + \".pkl\", \"rb\") as f:\n",
    "        loaded_logit_res[s] = pickle.load(f)\n",
    "    with open (\"cox_res_\" + settings[s] + \".pkl\", \"rb\") as f:\n",
    "        loaded_cox_res[s] = pickle.load(f)\n",
    "    with open (\"classpos_time_corr\" + settings[s] + \".pkl\", \"rb\") as f:\n",
    "        loaded_classpos_time_corr[s] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_stats = defaultdict(list)\n",
    "cox_stats = defaultdict(list)\n",
    "\n",
    "for sampling in [\"aggregate\", \"byday\"]:\n",
    "    for shape in [0.5, 1.0, 1.5, 2.0]:\n",
    "        df_ = pd.DataFrame(loaded_logit_res[sampling][shape])\n",
    "        df_[\"bias\"] = df_[\"coef\"] - 0.27\n",
    "        df_[\"bias_sq\"] = df_[\"bias\"]**2\n",
    "        df_agg = df_.aggregate([\"mean\",\"std\",\"sem\"])\n",
    "\n",
    "        logit_stats[sampling].append({\n",
    "            \"shape\": shape,\n",
    "            \"coef_mean\": df_agg[\"coef\"][\"mean\"],\n",
    "            \"coef_sem\": df_agg[\"coef\"][\"sem\"], \n",
    "            \"CI_coverage\": df_agg[\"coverage\"][\"mean\"], \n",
    "            \"bias\": df_agg[\"bias\"][\"mean\"], \n",
    "            \"bias_sq\": df_agg[\"bias_sq\"][\"mean\"]\n",
    "        })\n",
    "\n",
    "        df_ = pd.DataFrame(loaded_cox_res[sampling][shape])\n",
    "        df_[\"bias\"] = df_[\"coef\"] - 0.27\n",
    "        df_[\"bias_sq\"] = df_[\"bias\"]**2\n",
    "        df_agg = df_.aggregate([\"mean\",\"std\",\"sem\"])\n",
    "\n",
    "        cox_stats[sampling].append({\n",
    "            \"shape\": shape,\n",
    "            \"coef_mean\": df_agg[\"coef\"][\"mean\"],\n",
    "            \"coef_sem\": df_agg[\"coef\"][\"sem\"], \n",
    "            \"CI_coverage\": df_agg[\"coverage\"][\"mean\"], \n",
    "            \"bias\": df_agg[\"bias\"][\"mean\"], \n",
    "            \"bias_sq\": df_agg[\"bias_sq\"][\"mean\"]\n",
    "        })\n",
    "\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shape</th>\n",
       "      <th>coef_mean</th>\n",
       "      <th>coef_sem</th>\n",
       "      <th>CI_coverage</th>\n",
       "      <th>bias</th>\n",
       "      <th>bias_sq</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.109837</td>\n",
       "      <td>0.372707</td>\n",
       "      <td>0.98</td>\n",
       "      <td>-0.160163</td>\n",
       "      <td>6.832272</td>\n",
       "      <td>cox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.436017</td>\n",
       "      <td>0.194462</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.166017</td>\n",
       "      <td>1.880514</td>\n",
       "      <td>cox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.290355</td>\n",
       "      <td>0.098291</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.020355</td>\n",
       "      <td>0.473810</td>\n",
       "      <td>cox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.330368</td>\n",
       "      <td>0.048882</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.060368</td>\n",
       "      <td>0.120729</td>\n",
       "      <td>cox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.109977</td>\n",
       "      <td>0.372863</td>\n",
       "      <td>0.98</td>\n",
       "      <td>-0.160023</td>\n",
       "      <td>6.837905</td>\n",
       "      <td>logistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.436388</td>\n",
       "      <td>0.194607</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.166388</td>\n",
       "      <td>1.883400</td>\n",
       "      <td>logistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.290679</td>\n",
       "      <td>0.098409</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.020679</td>\n",
       "      <td>0.474957</td>\n",
       "      <td>logistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.330122</td>\n",
       "      <td>0.048747</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.060122</td>\n",
       "      <td>0.120052</td>\n",
       "      <td>logistic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shape  coef_mean  coef_sem  CI_coverage      bias   bias_sq     model\n",
       "0    0.5   0.109837  0.372707         0.98 -0.160163  6.832272       cox\n",
       "1    1.0   0.436017  0.194462         0.94  0.166017  1.880514       cox\n",
       "2    1.5   0.290355  0.098291         0.98  0.020355  0.473810       cox\n",
       "3    2.0   0.330368  0.048882         0.96  0.060368  0.120729       cox\n",
       "0    0.5   0.109977  0.372863         0.98 -0.160023  6.837905  logistic\n",
       "1    1.0   0.436388  0.194607         0.94  0.166388  1.883400  logistic\n",
       "2    1.5   0.290679  0.098409         0.98  0.020679  0.474957  logistic\n",
       "3    2.0   0.330122  0.048747         0.96  0.060122  0.120052  logistic"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_l = pd.DataFrame(logit_stats[\"aggregate\"])\n",
    "df_l[\"model\"] = \"logistic\"\n",
    "\n",
    "df_c = pd.DataFrame(cox_stats[\"aggregate\"])\n",
    "df_c[\"model\"] = \"cox\"\n",
    "\n",
    "df_cl = pd.concat([df_c, df_l])\n",
    "df_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shape</th>\n",
       "      <th>coef_mean</th>\n",
       "      <th>coef_sem</th>\n",
       "      <th>CI_coverage</th>\n",
       "      <th>bias</th>\n",
       "      <th>bias_sq</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.067122</td>\n",
       "      <td>0.458096</td>\n",
       "      <td>0.96</td>\n",
       "      <td>-0.202878</td>\n",
       "      <td>10.323886</td>\n",
       "      <td>cox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.310221</td>\n",
       "      <td>0.163327</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.040221</td>\n",
       "      <td>1.308732</td>\n",
       "      <td>cox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.142331</td>\n",
       "      <td>0.075528</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.127669</td>\n",
       "      <td>0.295818</td>\n",
       "      <td>cox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.288846</td>\n",
       "      <td>0.042812</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.018846</td>\n",
       "      <td>0.090164</td>\n",
       "      <td>cox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.793894</td>\n",
       "      <td>0.442724</td>\n",
       "      <td>0.94</td>\n",
       "      <td>-1.063894</td>\n",
       "      <td>10.736092</td>\n",
       "      <td>logistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.336570</td>\n",
       "      <td>0.155345</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.066570</td>\n",
       "      <td>1.186899</td>\n",
       "      <td>logistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.715266</td>\n",
       "      <td>0.077261</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.445266</td>\n",
       "      <td>0.490753</td>\n",
       "      <td>logistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.214583</td>\n",
       "      <td>0.044219</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.944583</td>\n",
       "      <td>0.988045</td>\n",
       "      <td>logistic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shape  coef_mean  coef_sem  CI_coverage      bias    bias_sq     model\n",
       "0    0.5   0.067122  0.458096         0.96 -0.202878  10.323886       cox\n",
       "1    1.0   0.310221  0.163327         0.96  0.040221   1.308732       cox\n",
       "2    1.5   0.142331  0.075528         1.00 -0.127669   0.295818       cox\n",
       "3    2.0   0.288846  0.042812         0.98  0.018846   0.090164       cox\n",
       "0    0.5  -0.793894  0.442724         0.94 -1.063894  10.736092  logistic\n",
       "1    1.0   0.336570  0.155345         0.96  0.066570   1.186899  logistic\n",
       "2    1.5   0.715266  0.077261         0.86  0.445266   0.490753  logistic\n",
       "3    2.0   1.214583  0.044219         0.16  0.944583   0.988045  logistic"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_l = pd.DataFrame(logit_stats[\"byday\"])\n",
    "df_l[\"model\"] = \"logistic\"\n",
    "\n",
    "df_c = pd.DataFrame(cox_stats[\"byday\"])\n",
    "df_c[\"model\"] = \"cox\"\n",
    "\n",
    "df_cl = pd.concat([df_c, df_l])\n",
    "df_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shape</th>\n",
       "      <th>coef_mean</th>\n",
       "      <th>coef_sem</th>\n",
       "      <th>CI_coverage</th>\n",
       "      <th>bias</th>\n",
       "      <th>bias_sq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.793894</td>\n",
       "      <td>0.442724</td>\n",
       "      <td>0.94</td>\n",
       "      <td>-1.063894</td>\n",
       "      <td>10.736092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.336570</td>\n",
       "      <td>0.155345</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.066570</td>\n",
       "      <td>1.186899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.715266</td>\n",
       "      <td>0.077261</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.445266</td>\n",
       "      <td>0.490753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.214583</td>\n",
       "      <td>0.044219</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.944583</td>\n",
       "      <td>0.988045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shape  coef_mean  coef_sem  CI_coverage      bias    bias_sq\n",
       "0    0.5  -0.793894  0.442724         0.94 -1.063894  10.736092\n",
       "1    1.0   0.336570  0.155345         0.96  0.066570   1.186899\n",
       "2    1.5   0.715266  0.077261         0.86  0.445266   0.490753\n",
       "3    2.0   1.214583  0.044219         0.16  0.944583   0.988045"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(logit_stats[\"byday\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shape</th>\n",
       "      <th>coef_mean</th>\n",
       "      <th>coef_sem</th>\n",
       "      <th>CI_coverage</th>\n",
       "      <th>bias</th>\n",
       "      <th>bias_sq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.109837</td>\n",
       "      <td>0.372707</td>\n",
       "      <td>0.98</td>\n",
       "      <td>-0.160163</td>\n",
       "      <td>6.832272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.436017</td>\n",
       "      <td>0.194462</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.166017</td>\n",
       "      <td>1.880514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.290355</td>\n",
       "      <td>0.098291</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.020355</td>\n",
       "      <td>0.473810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.330368</td>\n",
       "      <td>0.048882</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.060368</td>\n",
       "      <td>0.120729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shape  coef_mean  coef_sem  CI_coverage      bias   bias_sq\n",
       "0    0.5   0.109837  0.372707         0.98 -0.160163  6.832272\n",
       "1    1.0   0.436017  0.194462         0.94  0.166017  1.880514\n",
       "2    1.5   0.290355  0.098291         0.98  0.020355  0.473810\n",
       "3    2.0   0.330368  0.048882         0.96  0.060368  0.120729"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cox_stats[\"aggregate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shape</th>\n",
       "      <th>coef_mean</th>\n",
       "      <th>coef_sem</th>\n",
       "      <th>CI_coverage</th>\n",
       "      <th>bias</th>\n",
       "      <th>bias_sq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.067122</td>\n",
       "      <td>0.458096</td>\n",
       "      <td>0.96</td>\n",
       "      <td>-0.202878</td>\n",
       "      <td>10.323886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.310221</td>\n",
       "      <td>0.163327</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.040221</td>\n",
       "      <td>1.308732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.142331</td>\n",
       "      <td>0.075528</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.127669</td>\n",
       "      <td>0.295818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.288846</td>\n",
       "      <td>0.042812</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.018846</td>\n",
       "      <td>0.090164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shape  coef_mean  coef_sem  CI_coverage      bias    bias_sq\n",
       "0    0.5   0.067122  0.458096         0.96 -0.202878  10.323886\n",
       "1    1.0   0.310221  0.163327         0.96  0.040221   1.308732\n",
       "2    1.5   0.142331  0.075528         1.00 -0.127669   0.295818\n",
       "3    2.0   0.288846  0.042812         0.98  0.018846   0.090164"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cox_stats[\"byday\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x350 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams[\"font.family\"] = 'serif'\n",
    "fig, axs = plt.subplots(1,2,figsize=(10,3.5),constrained_layout=True)\n",
    "\n",
    "title_mappings = {\n",
    "    \"aggregate\": \"Time-invariant\",\n",
    "    \"byday\": \"Time-dependent\"\n",
    "}\n",
    "\n",
    "\n",
    "for i, sampling in enumerate([\"byday\", \"aggregate\"]):\n",
    "\n",
    "    df_ = pd.DataFrame(cox_stats[sampling])\n",
    "    axs[i].errorbar(\n",
    "        df_[\"shape\"], df_[\"coef_mean\"], yerr=1.96*df_[\"coef_sem\"],fmt=\"o\", capsize=3, label=\"cox\"\n",
    "    )\n",
    "\n",
    "    df_ = pd.DataFrame(logit_stats[sampling])\n",
    "    axs[i].errorbar(\n",
    "        df_[\"shape\"] + 0.05, df_[\"coef_mean\"], yerr=1.96*df_[\"coef_sem\"],fmt=\"^\", capsize=3, label=\"logistic\"\n",
    "    )\n",
    "\n",
    "    axs[i].hlines(0.27, 0.4, 2.1, colors=\"grey\", linestyles=\"dashed\")\n",
    "\n",
    "    axs[i].set_xticks([0.5, 1.0, 1.5, 2.0])\n",
    "    axs[i].set_title(title_mappings[sampling] + \" $\\it{ClassExposure}$\", fontsize=15)\n",
    "    axs[i].legend(loc=\"lower right\")\n",
    "    axs[i].set_xlabel(\"Shape ${\\\\nu}$\", fontsize=13)\n",
    "    axs[i].set_ylabel(\"Estimated coefficient\", fontsize=13)\n",
    "    axs[i].set_ylim(-2.0, 1.5)\n",
    "\n",
    "plt.savefig(\"../plots/simulation.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0.5: [{'coef': -2.336672396678648,\n",
       "               'p-value': 0.33539335801105863,\n",
       "               'coverage': True}],\n",
       "             1.0: [{'coef': -0.1432608568509429,\n",
       "               'p-value': 0.9004385394485179,\n",
       "               'coverage': True}],\n",
       "             1.5: [{'coef': 0.2078305020593834,\n",
       "               'p-value': 0.7478800210776722,\n",
       "               'coverage': True}],\n",
       "             2: [{'coef': 0.35564616816661865,\n",
       "               'p-value': 0.2997295991772331,\n",
       "               'coverage': True}]})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logit_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.DataFrame(loaded_logit_res[\"aggregate\"][0.5])\n",
    "df_[\"bias\"] = df_[\"coef\"] - 0.27\n",
    "df_[\"bias_sq\"] = df_[\"bias\"]**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>p-value</th>\n",
       "      <th>coverage</th>\n",
       "      <th>bias</th>\n",
       "      <th>bias_sq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.572790</td>\n",
       "      <td>0.512953</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.842790</td>\n",
       "      <td>7.114084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.582753</td>\n",
       "      <td>0.333330</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.582753</td>\n",
       "      <td>8.247334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sem</th>\n",
       "      <td>0.516551</td>\n",
       "      <td>0.066666</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.516551</td>\n",
       "      <td>1.649467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          coef   p-value  coverage      bias   bias_sq\n",
       "mean -0.572790  0.512953       1.0 -0.842790  7.114084\n",
       "std   2.582753  0.333330       0.0  2.582753  8.247334\n",
       "sem   0.516551  0.066666       0.0  0.516551  1.649467"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_agg = df_.aggregate([\"mean\",\"std\",\"sem\"])\n",
    "df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hazard(shape, scale=0.001):\n",
    "    x = np.arange(1,15)\n",
    "    y = scale * shape * x**(shape-1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "for shape in [0.5, 1.0, 1.5, 2.0]:\n",
    "    plt.plot(hazard(shape), label=\"shape=\"+str(shape))\n",
    "\n",
    "# plt.yscale(\"log\")\n",
    "plt.xlabel(\"Day $t$\", fontsize=12)\n",
    "plt.ylabel(\"Baseline hazard $h_0(t)$\", fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.title(\"Simulated baseline hazard\", fontsize=15)\n",
    "\n",
    "plt.savefig(\"../plots/simulated_baseline_hazard.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
